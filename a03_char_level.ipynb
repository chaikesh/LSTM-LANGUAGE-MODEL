{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import pickle\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.optimizers import RMSprop\n",
    "import io\n",
    "from pickle import dump\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "tokens_raw1 = gutenberg.raw('austen-emma.txt').lower()\n",
    "n1 = len(tokens_raw1)\n",
    "tokens_train1  = tokens_raw1[0:int(0.8*n1)];\n",
    "tokens_test1 = tokens_raw1[int(0.8*n1):n1]; \n",
    "\n",
    "tokens_raw2 = gutenberg.raw('austen-sense.txt').lower()\n",
    "n2 = len(tokens_raw2)\n",
    "tokens_train2  = tokens_raw2[0:int(0.8*n2)];\n",
    "tokens_test2 = tokens_raw2[int(0.8*n2):n2]; \n",
    "\n",
    "tokens_raw = tokens_train1 + tokens_train2\n",
    "tokens_test = tokens_test1 + tokens_test2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'\\n', u' ', u'!', u'\"', u'&', u\"'\", u'(', u')', u'*', u',', u'-', u'.', u'0', u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u':', u';', u'?', u'[', u']', u'_', u'`', u'a', u'b', u'c', u'd', u'e', u'f', u'g', u'h', u'i', u'j', u'k', u'l', u'm', u'n', u'o', u'p', u'q', u'r', u's', u't', u'u', u'v', u'w', u'x', u'y', u'z']\n",
      "55\n",
      "[u'm', u'm', u'a', u' ', u'b', u'y', u' ', u'j', u'a', u'n', u'e', u' ', u'a', u'u', u's', u't', u'e', u'n', u' ', u'1', u'8', u'1', u'6', u']', ' ', ' ', u'v', u'o', u'l', u'u', u'm', u'e', u' ', u'i', ' ', ' ', u'c', u'h', u'a', u'p', u't']\n",
      "Total Sequences: 624016\n"
     ]
    }
   ],
   "source": [
    "char_set = sorted(list(set(tokens_raw)))\n",
    "\n",
    "print(char_set)\n",
    "print(len(char_set))\n",
    "char_index_dict={}\n",
    "index_char_dict={}\n",
    "for i in range(len(char_set)):\n",
    "    char_index_dict[char_set[i]]=i\n",
    "    index_char_dict[i]=char_set[i]\n",
    "\n",
    "tokens_raw=list(tokens_raw)\n",
    "\n",
    "char_tr=[]\n",
    "for t in tokens_raw:\n",
    "    if(t!='\\n'):\n",
    "        char_tr.append(t)\n",
    "    else:\n",
    "        char_tr.append(\" \")\n",
    "        \n",
    "\n",
    "\n",
    "length = 41\n",
    "sequences =[]\n",
    "for i in range(length, len(char_tr),2):\n",
    "    seq = char_tr[i-length:i]\n",
    "    sequences.append(seq)\n",
    "print(sequences[1])\n",
    "lines = sequences;\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "sequences = tokenizer.texts_to_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(624016, 40)\n",
      "(624016, 1)\n"
     ]
    }
   ],
   "source": [
    "#creating tokens of 40 charactars to predict 41st charactar\n",
    "X=[]\n",
    "y=[]\n",
    "for i in range(len(sequences)):\n",
    "    X.append(sequences[i][:-1])\n",
    "    y.append(sequences[i][-1])\n",
    "X=np.array(X)\n",
    "y=np.array(y).reshape(-1,1)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 50)            2750      \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 40, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 55)                5555      \n",
      "=================================================================\n",
      "Total params: 159,205\n",
      "Trainable params: 159,205\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=40))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "624016/624016 [==============================] - 787s 1ms/step - loss: 2.1955 - acc: 0.3643\n",
      "Epoch 2/25\n",
      "624016/624016 [==============================] - 782s 1ms/step - loss: 1.7100 - acc: 0.4879\n",
      "Epoch 3/25\n",
      "624016/624016 [==============================] - 782s 1ms/step - loss: 1.5484 - acc: 0.5316\n",
      "Epoch 4/25\n",
      "624016/624016 [==============================] - 783s 1ms/step - loss: 1.4582 - acc: 0.5567\n",
      "Epoch 5/25\n",
      "624016/624016 [==============================] - 783s 1ms/step - loss: 1.4010 - acc: 0.5715\n",
      "Epoch 6/25\n",
      "624016/624016 [==============================] - 784s 1ms/step - loss: 1.3601 - acc: 0.5826\n",
      "Epoch 7/25\n",
      "624016/624016 [==============================] - 784s 1ms/step - loss: 1.3290 - acc: 0.5901\n",
      "Epoch 8/25\n",
      "624016/624016 [==============================] - 785s 1ms/step - loss: 1.3039 - acc: 0.5968\n",
      "Epoch 9/25\n",
      "624016/624016 [==============================] - 785s 1ms/step - loss: 1.2841 - acc: 0.6020\n",
      "Epoch 10/25\n",
      "624016/624016 [==============================] - 786s 1ms/step - loss: 1.2679 - acc: 0.6065\n",
      "Epoch 11/25\n",
      "624016/624016 [==============================] - 786s 1ms/step - loss: 1.2529 - acc: 0.6101\n",
      "Epoch 12/25\n",
      "624016/624016 [==============================] - 786s 1ms/step - loss: 1.2411 - acc: 0.6137\n",
      "Epoch 13/25\n",
      "624016/624016 [==============================] - 787s 1ms/step - loss: 1.2307 - acc: 0.6162\n",
      "Epoch 14/25\n",
      "624016/624016 [==============================] - 787s 1ms/step - loss: 1.2210 - acc: 0.6183\n",
      "Epoch 15/25\n",
      "624016/624016 [==============================] - 787s 1ms/step - loss: 1.2123 - acc: 0.6206\n",
      "Epoch 16/25\n",
      "624016/624016 [==============================] - 787s 1ms/step - loss: 1.2049 - acc: 0.6227\n",
      "Epoch 17/25\n",
      "624016/624016 [==============================] - 788s 1ms/step - loss: 1.1999 - acc: 0.6237\n",
      "Epoch 18/25\n",
      "624016/624016 [==============================] - 788s 1ms/step - loss: 1.1920 - acc: 0.6253\n",
      "Epoch 19/25\n",
      "624016/624016 [==============================] - 788s 1ms/step - loss: 1.1863 - acc: 0.6280\n",
      "Epoch 20/25\n",
      "624016/624016 [==============================] - 789s 1ms/step - loss: 1.1806 - acc: 0.6286\n",
      "Epoch 21/25\n",
      "624016/624016 [==============================] - 789s 1ms/step - loss: 1.1757 - acc: 0.6300\n",
      "Epoch 22/25\n",
      "624016/624016 [==============================] - 790s 1ms/step - loss: 1.1704 - acc: 0.6314\n",
      "Epoch 23/25\n",
      "624016/624016 [==============================] - 791s 1ms/step - loss: 1.1662 - acc: 0.6326\n",
      "Epoch 24/25\n",
      "624016/624016 [==============================] - 791s 1ms/step - loss: 1.1618 - acc: 0.6331\n",
      "Epoch 25/25\n",
      "624016/624016 [==============================] - 791s 1ms/step - loss: 1.1574 - acc: 0.6352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc5fe045d10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X,y, batch_size=200, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the model to file\n",
    "model.save('model_a03_cl.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer_a03_cl.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "# load the model\n",
    "model = load_model('model_a03_cl.h5')\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer_a03_cl.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[]\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        #yhat1 = model.predict(encoded, verbose=0)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text.append(out_word)\n",
    "        result.append(out_word)\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 28, 1, 5, 6, 11, 1, 3, 9, 2, 1, 10, 2, 14, 2, 14, 21, 10, 5, 6, 17, 2, 1, 4, 18, 1, 5, 12, 12, 1, 9, 2, 10, 1, 18, 4, 10, 14, 2, 10, 1]\n",
      "[u'e', u';', u' ', u'a', u'n', u'd', u' ', u't', u'h', u'e', u' ', u'r', u'e', u'm', u'e', u'm', u'b', u'r', u'a', u'n', u'c', u'e', ' ', u'o', u'f', u' ', u'a', u'l', u'l', u' ', u'h', u'e', u'r', u' ', u'f', u'o', u'r', u'm', u'e', u'r', u' ', u't']\n"
     ]
    }
   ],
   "source": [
    "#seed_text = lines[randint(0,len(lines))]\n",
    "in_text = seed_text\n",
    "encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "print(encoded)\n",
    "encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "yhat = model.predict_classes(encoded, verbose=0)\n",
    "out_word = ''\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index == yhat:\n",
    "        out_word = word\n",
    "        break\n",
    "                \n",
    "in_text.append(out_word)\n",
    "print(in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all the tediousness of the many years of \n",
      "the same old from the conscious and seeing the door, and they were a serious of \n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(''.join(seed_text) + '')\n",
    "generated = generate_seq(model, tokenizer, length-1, seed_text, 80)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "#perplexity\n",
    "seq_length = length-1\n",
    "test_tokens = tokens_test[1:10000]\n",
    "\n",
    "in_text = [test_tokens[0]]\n",
    "logp = 0\n",
    "c=0\n",
    "for word in test_tokens:\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    \n",
    "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "    #print encoded\n",
    "    yhat1 = model.predict(encoded,verbose=0)[0]\n",
    "    next_word = word\n",
    "    if next_word in tokenizer.word_index:\n",
    "        p = yhat1[tokenizer.word_index[word]]\n",
    "        if  p !=0:\n",
    "            c+=1\n",
    "            logp = logp + np.log(p)\n",
    "\n",
    "    else:\n",
    "        p = 1\n",
    "        \n",
    "        c+=1\n",
    "        logp = logp + np.log(p)\n",
    "    \n",
    "    in_text.append(word)\n",
    "    #print(in_text)\n",
    "    \n",
    "pplxty = np.exp(-1*logp/c)\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.569143307682693"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pplxty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
