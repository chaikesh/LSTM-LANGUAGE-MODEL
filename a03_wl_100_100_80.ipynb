{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-pNREKOX_6mm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaikesh/anaconda2/envs/tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66802"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[]\n",
    "from random import randint\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "import numpy as np\n",
    "# In[]\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # replace '--' with a space ' '\n",
    "    doc = doc.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "\n",
    "\n",
    "    tokens = [re.sub(r'[^\\w\\s]','',w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# In[]\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "tokens_raw1 = gutenberg.words('austen-emma.txt')\n",
    "n1 = len(tokens_raw1)\n",
    "tokens_train1  = tokens_raw1[0:int(0.8*n1)];\n",
    "tokens_test1 = tokens_raw1[int(0.8*n1):n1]; \n",
    "\n",
    "tokens_raw2 = gutenberg.words('austen-sense.txt')\n",
    "n2 = len(tokens_raw2)\n",
    "tokens_train2  = tokens_raw2[0:int(0.8*n2)];\n",
    "tokens_test2 = tokens_raw2[int(0.8*n2):n2]; \n",
    "\n",
    "tokens_raw = tokens_train1 + tokens_train2\n",
    "tokens_test = tokens_test1 + tokens_test2\n",
    "\n",
    "text_raw = ' '.join(tokens_raw)\n",
    "tokens = clean_doc(text_raw)\n",
    "\n",
    "\n",
    "\n",
    "len(tokens_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LB5gs9hLM3ru"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 112954\n"
     ]
    }
   ],
   "source": [
    "# In[]\n",
    "# organize into sequences of tokens\n",
    "length = 50 + 1\n",
    "step =2\n",
    "sequences = list()\n",
    "for i in range(0,len(tokens)-length, step):\n",
    "\t# select sequence of tokens\n",
    "\tseq = tokens[i:i+length]\n",
    "\t# convert into a line\n",
    "\tline = ' '.join(seq)\n",
    "\t# store\n",
    "\tsequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZQMb-7F1MuE0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sequences\n",
    "# In[]\n",
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# In[]\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# In[]\n",
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "I0GkyqlTMerN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 100)           827800    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           80400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8278)              836078    \n",
      "=================================================================\n",
      "Total params: 1,834,778\n",
      "Trainable params: 1,834,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "I0GkyqlTMerN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "112954/112954 [==============================] - 299s 3ms/step - loss: 6.3801 - acc: 0.0396\n",
      "Epoch 2/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 5.9310 - acc: 0.0655\n",
      "Epoch 3/80\n",
      "112954/112954 [==============================] - 315s 3ms/step - loss: 5.6631 - acc: 0.0880\n",
      "Epoch 4/80\n",
      "112954/112954 [==============================] - 315s 3ms/step - loss: 5.4777 - acc: 0.1011\n",
      "Epoch 5/80\n",
      "112954/112954 [==============================] - 315s 3ms/step - loss: 5.3665 - acc: 0.1084\n",
      "Epoch 6/80\n",
      "112954/112954 [==============================] - 315s 3ms/step - loss: 5.2642 - acc: 0.1154\n",
      "Epoch 7/80\n",
      "112954/112954 [==============================] - 315s 3ms/step - loss: 5.1539 - acc: 0.1246\n",
      "Epoch 8/80\n",
      "112954/112954 [==============================] - 315s 3ms/step - loss: 5.0569 - acc: 0.1294\n",
      "Epoch 9/80\n",
      "112954/112954 [==============================] - 315s 3ms/step - loss: 4.9822 - acc: 0.1344\n",
      "Epoch 10/80\n",
      "112954/112954 [==============================] - 315s 3ms/step - loss: 4.9083 - acc: 0.1381\n",
      "Epoch 11/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.8421 - acc: 0.1418\n",
      "Epoch 12/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.7572 - acc: 0.1463\n",
      "Epoch 13/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.6856 - acc: 0.1512\n",
      "Epoch 14/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.6195 - acc: 0.1532\n",
      "Epoch 15/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.5503 - acc: 0.1571\n",
      "Epoch 16/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.4960 - acc: 0.1606\n",
      "Epoch 17/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.4327 - acc: 0.1648\n",
      "Epoch 18/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.3671 - acc: 0.1690\n",
      "Epoch 19/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.3071 - acc: 0.1734\n",
      "Epoch 20/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.2488 - acc: 0.1775\n",
      "Epoch 21/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.1933 - acc: 0.1835\n",
      "Epoch 22/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.1625 - acc: 0.1867\n",
      "Epoch 23/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.1383 - acc: 0.1907\n",
      "Epoch 24/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.1551 - acc: 0.1894\n",
      "Epoch 25/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.1323 - acc: 0.1911\n",
      "Epoch 26/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.0857 - acc: 0.1944\n",
      "Epoch 27/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.0829 - acc: 0.1945\n",
      "Epoch 28/80\n",
      "112954/112954 [==============================] - 316s 3ms/step - loss: 4.0526 - acc: 0.1967\n",
      "Epoch 29/80\n",
      "112954/112954 [==============================] - 320s 3ms/step - loss: 3.9499 - acc: 0.2068\n",
      "Epoch 30/80\n",
      "112954/112954 [==============================] - 362s 3ms/step - loss: 3.8795 - acc: 0.2148\n",
      "Epoch 31/80\n",
      "112954/112954 [==============================] - 415s 4ms/step - loss: 3.8279 - acc: 0.2202\n",
      "Epoch 32/80\n",
      "112954/112954 [==============================] - 345s 3ms/step - loss: 3.7852 - acc: 0.2249\n",
      "Epoch 33/80\n",
      "112954/112954 [==============================] - 314s 3ms/step - loss: 3.7452 - acc: 0.2298\n",
      "Epoch 34/80\n",
      "112954/112954 [==============================] - 317s 3ms/step - loss: 3.7001 - acc: 0.2356\n",
      "Epoch 35/80\n",
      "112954/112954 [==============================] - 332s 3ms/step - loss: 3.6899 - acc: 0.2377\n",
      "Epoch 36/80\n",
      "112954/112954 [==============================] - 313s 3ms/step - loss: 3.7609 - acc: 0.2300\n",
      "Epoch 37/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.7472 - acc: 0.2319\n",
      "Epoch 38/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.6214 - acc: 0.2458\n",
      "Epoch 39/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.5548 - acc: 0.2554\n",
      "Epoch 40/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.5022 - acc: 0.2615\n",
      "Epoch 41/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.5086 - acc: 0.2624\n",
      "Epoch 42/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.6259 - acc: 0.2492\n",
      "Epoch 43/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.5660 - acc: 0.2549\n",
      "Epoch 44/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.4879 - acc: 0.2642\n",
      "Epoch 45/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.4640 - acc: 0.2689\n",
      "Epoch 46/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.4392 - acc: 0.2733\n",
      "Epoch 47/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.3891 - acc: 0.2801\n",
      "Epoch 48/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.3649 - acc: 0.2833\n",
      "Epoch 49/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.3378 - acc: 0.2873\n",
      "Epoch 50/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.2612 - acc: 0.2973\n",
      "Epoch 51/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.1928 - acc: 0.3078\n",
      "Epoch 52/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.2033 - acc: 0.3089\n",
      "Epoch 53/80\n",
      "112954/112954 [==============================] - 311s 3ms/step - loss: 3.1984 - acc: 0.3096\n",
      "Epoch 54/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 3.1593 - acc: 0.3169\n",
      "Epoch 55/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 3.1014 - acc: 0.3253\n",
      "Epoch 56/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 3.1458 - acc: 0.3206\n",
      "Epoch 57/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 3.1045 - acc: 0.3266\n",
      "Epoch 58/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 3.0398 - acc: 0.3368\n",
      "Epoch 59/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 3.0039 - acc: 0.3434\n",
      "Epoch 60/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 3.0115 - acc: 0.3432\n",
      "Epoch 61/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 2.9441 - acc: 0.3513\n",
      "Epoch 62/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 2.8790 - acc: 0.3623\n",
      "Epoch 63/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 2.8283 - acc: 0.3719\n",
      "Epoch 64/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 2.8529 - acc: 0.3701\n",
      "Epoch 65/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 3.0866 - acc: 0.3389\n",
      "Epoch 66/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 3.0887 - acc: 0.3380\n",
      "Epoch 67/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 2.9605 - acc: 0.3556\n",
      "Epoch 68/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 2.8826 - acc: 0.3653\n",
      "Epoch 69/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 2.8696 - acc: 0.3671\n",
      "Epoch 70/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 3.0088 - acc: 0.3436\n",
      "Epoch 71/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 2.8202 - acc: 0.3765\n",
      "Epoch 72/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 2.7593 - acc: 0.3883\n",
      "Epoch 73/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 2.7113 - acc: 0.3955\n",
      "Epoch 74/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 2.6800 - acc: 0.4014\n",
      "Epoch 75/80\n",
      "112954/112954 [==============================] - 310s 3ms/step - loss: 2.6700 - acc: 0.4033\n",
      "Epoch 76/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 2.6259 - acc: 0.4118\n",
      "Epoch 77/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 2.5905 - acc: 0.4176\n",
      "Epoch 78/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 2.8237 - acc: 0.3816\n",
      "Epoch 79/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 2.7325 - acc: 0.3993\n",
      "Epoch 80/80\n",
      "112954/112954 [==============================] - 309s 3ms/step - loss: 2.7395 - acc: 0.4005\n"
     ]
    }
   ],
   "source": [
    "# In[]\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=80)\n",
    "\n",
    "# In[]\n",
    "\n",
    "# save the model to file\n",
    "model.save('model_a03_wl_100_100_80.h5')\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer_a03_wl_100_100_80.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "# load the model\n",
    "model = load_model('model_a03_wl_100_100_80.h5')\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer_a03_wl_100_100_80.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "I0GkyqlTMerN"
   },
   "outputs": [],
   "source": [
    "# In[]\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# generate a sequence from a language model\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "\tresult = list()\n",
    "\tin_text = seed_text\n",
    "\t# generate a fixed number of words\n",
    "\tfor _ in range(n_words):\n",
    "\t\t# encode the text as integer\n",
    "\t\tencoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "\t\t# truncate sequences to a fixed length\n",
    "\t\tencoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "\t\t# predict probabilities for each word\n",
    "\t\tyhat = model.predict_classes(encoded, verbose=0)\n",
    "        #yhat1 = model.predict(encoded, verbose=0)\n",
    "\t\t# map predicted word index to word\n",
    "\t\tout_word = ''\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == yhat:\n",
    "\t\t\t\tout_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\t# append to input\n",
    "\t\tin_text += ' ' + out_word\n",
    "\t\tresult.append(out_word)\n",
    "\treturn ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Yy1OBGbKAIaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am as confident of seeing frank here before the middle of january as i am of being here myself but your good friend there nodding towards the upper end of the table has so few vagaries herself and has been so little used to them at hartfield that she cannot calculate\n",
      "\n",
      "yourself at randalls though i am sure i am sure you will be very much\n"
     ]
    }
   ],
   "source": [
    "# select a seed text\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "print(seed_text + '\\n')\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 15)\n",
    "print(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LOibiu6EJJV0"
   },
   "outputs": [],
   "source": [
    "#perplexity\n",
    "\n",
    "test_tokens = tokens_test[1:10000]\n",
    "in_text = test_tokens[0]\n",
    "logp = 0\n",
    "c=0\n",
    "for word in test_tokens:\n",
    "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "    yhat1 = model.predict(encoded,verbose=0)[0]\n",
    "    next_word = word\n",
    "    if next_word in tokenizer.word_index:\n",
    "        p = yhat1[tokenizer.word_index[next_word]]\n",
    "        if  p >=0.00001:\n",
    "            c+=1\n",
    "            logp = logp + np.log(p)\n",
    "\n",
    "    else:\n",
    "        p = 1\n",
    "        c+=1\n",
    "        logp = logp + np.log(p)\n",
    "    \n",
    "    in_text +=' ' + next_word\n",
    "pplxty = np.exp(-1*logp/c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "u1kS54TaJXCx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.494929009315772"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pplxty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aV8_uO8hJ7Jo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "a03_wl_300_100_80_singlelayer.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
